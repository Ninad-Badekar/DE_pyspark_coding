{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4504663",
   "metadata": {},
   "source": [
    "<pre>Problem 1: PySpark – Calculate Session Gaps for Users\n",
    "Problem Statement\n",
    "\n",
    "You have a PySpark DataFrame with user session information. Each row represents a session start date for a user. Write a PySpark program to calculate the gap in days between consecutive sessions for each user.\n",
    "Sample Input (user_sessions)\n",
    "user_id \tsession_date\n",
    "U1 \t2025-01-01\n",
    "U1 \t2025-01-05\n",
    "U1 \t2025-01-10\n",
    "U2 \t2025-01-02\n",
    "U2 \t2025-01-04\n",
    "Expected Output\n",
    "user_id \tsession_date \tdays_since_last_session\n",
    "U1 \t2025-01-01 \tNULL\n",
    "U1 \t2025-01-05 \t4\n",
    "U1 \t2025-01-10 \t5\n",
    "U2 \t2025-01-02 \tNULL\n",
    "U2 \t2025-01-04 \t2\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e788ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6a28fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"U1\", \t\"2025-01-01\"),\n",
    "(\"U1\" ,\t\"2025-01-05\"),\n",
    "(\"U1\" ,\t\"2025-01-10\"),\n",
    "(\"U2\" ,\t\"2025-01-02\"),\n",
    "(\"U2\" \t,\"2025-01-04\")]\n",
    "schema = [\"user_id\",\"session_date\"]\n",
    "spark = SparkSession.builder.appName(\"Daily-Day7\").getOrCreate()\n",
    "user_sessions = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5eff5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+\n",
      "|user_id|session_date|gap_days|\n",
      "+-------+------------+--------+\n",
      "|     U1|  2025-01-01|    NULL|\n",
      "|     U1|  2025-01-05|       4|\n",
      "|     U1|  2025-01-10|       5|\n",
      "|     U2|  2025-01-02|    NULL|\n",
      "|     U2|  2025-01-04|       2|\n",
      "+-------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"user_id\").orderBy(\"session_date\")\n",
    "user_sessions = user_sessions.withColumn(\"prev_session_date\", F.lag(\"session_date\").over(windowSpec))\n",
    "user_sessions = user_sessions.withColumn(\"gap_days\", \n",
    "    F.when(F.col(\"prev_session_date\").isNull(), None)\n",
    "     .otherwise(F.datediff(F.col(\"session_date\"), F.col(\"prev_session_date\"))))\n",
    "result = user_sessions.select(\"user_id\", \"session_date\", \"gap_days\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ccdde6",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Problem 2: SQL – Find Customers with Only One Purchase\n",
    "Problem Statement\n",
    "\n",
    "You have a SQL table purchases(customer_id, purchase_date, amount) representing customer purchases. Write a SQL query to find all customers who made exactly one purchase.\n",
    "Sample Input (purchases)\n",
    "customer_id \tpurchase_date \tamount\n",
    "C1 \t2025-01-01 \t200\n",
    "C1 \t2025-01-10 \t300\n",
    "C2 \t2025-01-05 \t150\n",
    "C3 \t2025-01-02 \t100\n",
    "C3 \t2025-01-04 \t200\n",
    "Expected Output\n",
    "customer_id\n",
    "C2\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02ffa982",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"C1\", \t\"2025-01-01\", \t200),\n",
    "    (\"C1\", \t\"2025-01-10\" ,\t300),\n",
    "    (\"C2\", \t\"2025-01-05\", \t150),\n",
    "    (\"C3\", \t\"2025-01-02\" ,\t100),\n",
    "    (\"C3\", \t\"2025-01-04\", \t200)\n",
    "]\n",
    "schema = [\"customer_id\", \"purchase_date\",\"amount\"]\n",
    "purchases = spark.createDataFrame(data, schema)\n",
    "purchases = purchases.withColumn(\"purchase_date\", F.to_date(F.col(\"purchase_date\"), \"yyyy-MM-dd\"))\n",
    "purchases.createOrReplaceTempView(\"purchases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc94626c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|         C2|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"select customer_id\n",
    "from purchases \n",
    "group by customer_id\n",
    "having count(*) = 1\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186dd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
