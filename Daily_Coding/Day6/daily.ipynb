{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f45a0a7",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Problem Statement\n",
    "\n",
    "You have a PySpark DataFrame containing employee salaries across departments. Write a PySpark program to rank employees within each department based on salary in descending order.\n",
    "Sample Input (employee_salaries)\n",
    "emp_id \tdept \tsalary\n",
    "1 \tHR \t60000\n",
    "2 \tHR \t75000\n",
    "3 \tHR \t50000\n",
    "4 \tIT \t90000\n",
    "5 \tIT \t85000\n",
    "Expected Output\n",
    "emp_id \tdept \tsalary \trank\n",
    "2 \tHR \t75000 \t1\n",
    "1 \tHR \t60000 \t2\n",
    "3 \tHR \t50000 \t3\n",
    "4 \tIT \t90000 \t1\n",
    "5 \tIT \t85000 \t2\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c8d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfe65c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/26 10:26:07 WARN Utils: Your hostname, neosoft-Latitude-5420 resolves to a loopback address: 127.0.1.1; using 10.0.61.174 instead (on interface wlp0s20f3)\n",
      "25/08/26 10:26:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/26 10:26:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Daily-Day6\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4309350",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"dept\").orderBy(F.desc(\"salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12aa5dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1 \t,'HR', \t60000),\n",
    "(2 ,\t'HR' ,\t75000),\n",
    "(3 ,\t'HR' ,\t50000),\n",
    "(4 \t,'IT' ,\t90000),\n",
    "(5 ,\t'IT', \t85000)]\n",
    "schema = [\"id\", \"dept\", \"salary\"]\n",
    "empolyee_salaries = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3dea0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+----+\n",
      "| id|dept|salary|rank|\n",
      "+---+----+------+----+\n",
      "|  2|  HR| 75000|   1|\n",
      "|  1|  HR| 60000|   2|\n",
      "|  3|  HR| 50000|   3|\n",
      "|  4|  IT| 90000|   1|\n",
      "|  5|  IT| 85000|   2|\n",
      "+---+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_rank = empolyee_salaries.withColumn(\"rank\", F.rank().over(windowSpec))\n",
    "salary_rank.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed30fe4",
   "metadata": {},
   "source": [
    "<pre>Problem Statement\n",
    "\n",
    "You have a table product_sales(product_id, sale_date) representing product sales dates. Write a SQL query to find products that were sold in every month of 2025.\n",
    "Sample Input (product_sales)\n",
    "product_id \tsale_date\n",
    "P1 \t2025-01-10\n",
    "P1 \t2025-02-15\n",
    "P1 \t2025-03-20\n",
    "P2 \t2025-01-05\n",
    "P2 \t2025-02-10\n",
    "Expected Output\n",
    "product_id\n",
    "P1\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54ae6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"P1\", \"2025-01-10\"),\n",
    "    (\"P1\", \"2025-02-12\"),\n",
    "    (\"P1\", \"2025-03-09\"),\n",
    "    (\"P1\", \"2025-04-18\"),\n",
    "    (\"P1\", \"2025-05-03\"),\n",
    "    (\"P1\", \"2025-06-27\"),\n",
    "    (\"P1\", \"2025-07-14\"),\n",
    "    (\"P1\", \"2025-08-21\"),\n",
    "    (\"P1\", \"2025-09-02\"),\n",
    "    (\"P1\", \"2025-10-11\"),\n",
    "    (\"P1\", \"2025-11-06\"),\n",
    "    (\"P1\", \"2025-12-05\"),\n",
    "    (\"P2\", \"2025-01-05\"),\n",
    "    (\"P2\", \"2025-02-10\"),\n",
    "    (\"P2\", \"2025-03-15\"),\n",
    "    (\"P2\", \"2025-05-20\"),\n",
    "    (\"P2\", \"2025-06-08\"),\n",
    "    (\"P2\", \"2025-07-22\"),\n",
    "    (\"P2\", \"2025-08-30\"),\n",
    "    (\"P2\", \"2025-10-01\"),\n",
    "    (\"P2\", \"2025-11-19\"),\n",
    "    (\"P2\", \"2025-12-07\"),\n",
    "    (\"P3\", \"2025-01-02\"),\n",
    "    (\"P3\", \"2025-02-14\"),\n",
    "    (\"P3\", \"2025-03-03\"),\n",
    "    (\"P3\", \"2025-04-25\"),\n",
    "    (\"P3\", \"2025-05-09\"),\n",
    "    (\"P3\", \"2025-06-16\"),\n",
    "    (\"P3\", \"2025-07-07\"),\n",
    "    (\"P3\", \"2025-08-12\"),\n",
    "    (\"P3\", \"2025-09-28\"),\n",
    "    (\"P3\", \"2025-10-20\"),\n",
    "    (\"P3\", \"2025-11-03\"),\n",
    "    (\"P3\", \"2025-12-29\"),\n",
    "]\n",
    "\n",
    "schema = [\"product_id\", \"sale_date\"]\n",
    "product_sales = spark.createDataFrame(data, schema)\n",
    "product_sales = product_sales.withColumn(\"sale_date\", F.to_date(F.col(\"sale_date\"), \"yyyy-MM-dd\"))\n",
    "product_sales.createOrReplaceTempView(\"product_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e21face1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|        P1|\n",
      "|        P3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"SELECT product_id\n",
    "FROM product_sales\n",
    "WHERE YEAR(sale_date) = 2025\n",
    "GROUP BY product_id\n",
    "HAVING COUNT(DISTINCT MONTH(sale_date)) = 12\n",
    "order BY product_id;\n",
    "\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc681c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
