{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964f274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "599a918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/18 15:25:02 WARN Utils: Your hostname, neosoft-Latitude-5420 resolves to a loopback address: 127.0.1.1; using 10.0.61.31 instead (on interface wlp0s20f3)\n",
      "25/08/18 15:25:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/18 15:25:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Day 1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6fac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"John\", 28), (\"Jane\", 33), (\"Mike\", 45)]\n",
    "cols = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9669469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|John| 28|\n",
      "|Jane| 33|\n",
      "|Mike| 45|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91562df",
   "metadata": {},
   "source": [
    "Filter rows where salary > 3000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc928e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,'Alice',3000),(2,'Bob',1500),(3,'Carol',4000)]\n",
    "cols=['id','name','salary']\n",
    "df = spark.createDataFrame(data,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12aa0489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  3|Carol|  4000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_df = df.filter(df.salary > 3000)\n",
    "filter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f726ac5",
   "metadata": {},
   "source": [
    " Add new column with literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fe2e2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+\n",
      "| id| name|salary|Flags|\n",
      "+---+-----+------+-----+\n",
      "|  3|Carol|  4000|    Y|\n",
      "+---+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_df= filter_df.withColumn(\"Flags\",lit(\"Y\"))\n",
    "filter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cdb122",
   "metadata": {},
   "source": [
    "Rename column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19a9633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+---+\n",
      "| id| name|salary|Yes|\n",
      "+---+-----+------+---+\n",
      "|  3|Carol|  4000|  Y|\n",
      "+---+-----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_df = filter_df.withColumnRenamed(\"Flags\",\"Yes\")\n",
    "filter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add0e62",
   "metadata": {},
   "source": [
    "Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a7074d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[('a',1),('b',2),('a',1)]\n",
    "cols=['k','v']\n",
    "df=spark.createDataFrame(data,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35b1c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  k|  v|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  b|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786ac5b",
   "metadata": {},
   "source": [
    "Count rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b691215c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da22b2",
   "metadata": {},
   "source": [
    "Select distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bcb5d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  k|\n",
      "+---+\n",
      "|  a|\n",
      "|  b|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"k\")).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08593f43",
   "metadata": {},
   "source": [
    "Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bc27ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item: string (nullable = true)\n",
      " |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[('p',10),('q',20),('p',5)]\n",
    "cols=['item','qty']\n",
    "df=spark.createDataFrame(data,cols)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c8252f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|item|totalQty|\n",
      "+----+--------+\n",
      "|   p|      15|\n",
      "|   q|      20|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "(df.groupBy('item').agg(sum('qty').alias('totalQty')).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46172dea",
   "metadata": {},
   "source": [
    "Convert to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a0c424c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>qty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  item  qty\n",
       "0    p   10\n",
       "1    q   20\n",
       "2    p    5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb905c",
   "metadata": {},
   "source": [
    "Order rows by score descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f844963",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[('p',3),('q',1),('r',2)]\n",
    "cols=['id','score']\n",
    "df=spark.createDataFrame(data,cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e941b86d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca71a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
